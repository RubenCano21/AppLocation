#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ETL con Spark - VersiÃ³n optimizada para cargar a BD destino
Usa el pipeline ETL original pero con configuraciones mejoradas
"""

import asyncio
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.services.etl_service import ETLService
from app.services.dimension_service import DimensionService
from app.database.postgres_db import get_db
import logging
import time

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

async def run_spark_etl_to_database():
    """ETL completo usando Spark para cargar a BD destino"""

    print("\n" + "â•”" + "â•"*80 + "â•—")
    print("â•‘" + " "*15 + "ETL SPARK - DESCENTRALIZACIÃ“N + CARGA A BD DESTINO" + " "*14 + "â•‘")
    print("â•š" + "â•"*80 + "â•")

    start_time = time.time()
    etl_service = None

    try:
        print("\nğŸ“Š PASO 1: Verificar/Poblar DimensiÃ³n de Tiempo")
        print("="*70)

        try:
            dimension_service = DimensionService()
            result = dimension_service.populate_dim_time()
            print(f"âœ… DimensiÃ³n dim_time: {result}")
        except Exception as e:
            print(f"âš ï¸ Error poblando dim_time (continuando): {e}")

        print("\nğŸ”§ PASO 2: Inicializar Spark ETL Service")
        print("="*70)

        etl_service = ETLService()
        print("âœ… Spark ETL Service inicializado")

        print("\nğŸ“¥ PASO 3: ExtracciÃ³n desde Supabase")
        print("="*70)
        print("  ğŸ“¡ Fuente: Supabase (API REST)")
        print("  ğŸ”„ MÃ©todo: PaginaciÃ³n basada en cursor")
        print("  ğŸ“¦ Procesando...")

        # Extraer datos con paginaciÃ³n mejorada
        data = await etl_service.extract_from_supabase()

        if not data:
            print("âŒ No se obtuvieron datos")
            return

        print(f"âœ… ExtracciÃ³n completa: {len(data):,} registros")

        print("\nğŸ”„ PASO 4: TransformaciÃ³n con Spark")
        print("="*70)
        print("  âš™ï¸ Procesando con PySpark...")
        print("  ğŸ• Aplicando descentralizaciÃ³n de timestamp...")
        print("  ğŸ§¹ Limpiando y validando datos...")
        print("  ğŸ“Š Generando agregaciones...")

        # Transformar con Spark
        df_transformed, df_grid, df_devices, statistics = etl_service.transform_with_spark(data)

        if df_transformed is None:
            print("âŒ Error en transformaciÃ³n Spark")
            return

        print("âœ… TransformaciÃ³n Spark completada")
        print(f"ğŸ“Š Registros transformados: {statistics['total_points']:,}")
        print(f"ğŸ“Š Dispositivos Ãºnicos: {statistics['unique_devices']}")
        print(f"ğŸ“Š Celdas de grilla: {df_grid.count()}")

        print("\nğŸ’¾ PASO 5: Carga a PostgreSQL")
        print("="*70)
        print("  ğŸ—„ï¸ Destino: PostgreSQL")
        print("  ğŸ”„ MÃ©todo: SQLAlchemy + pandas")
        print("  ğŸ“‹ Tablas a crear: locations, location_grid, device_stats")

        # Cargar tabla principal
        print("\nğŸ”¸ Cargando tabla principal 'locations'...")
        locations_count = etl_service.load_to_postgres(
            df_transformed,
            "locations",
            mode="overwrite"
        )

        print("\nğŸ”¸ Cargando agregaciones geogrÃ¡ficas 'location_grid'...")
        grid_count = etl_service.load_to_postgres(
            df_grid,
            "location_grid",
            mode="overwrite"
        )

        print("\nğŸ”¸ Cargando estadÃ­sticas de dispositivos 'device_stats'...")
        device_count = etl_service.load_to_postgres(
            df_devices,
            "device_stats",
            mode="overwrite"
        )

        elapsed = time.time() - start_time

        print(f"\n" + "ğŸ‰" + "="*78 + "ğŸ‰")
        print("                   Â¡ETL SPARK COMPLETADO EXITOSAMENTE!")
        print("="*80)
        print(f"ğŸ“Š Datos extraÃ­dos: {len(data):,}")
        print(f"ğŸ“Š Registros en locations: {locations_count:,}")
        print(f"ğŸ“Š Celdas en location_grid: {grid_count:,}")
        print(f"ğŸ“Š Dispositivos en device_stats: {device_count:,}")
        print(f"â±ï¸  Tiempo total: {elapsed:.1f} segundos")
        print(f"ğŸš€ Velocidad: {len(data)/elapsed:.0f} registros/segundo")
        print()
        print("âœ… CAMPOS DE DESCENTRALIZACIÃ“N DE TIMESTAMP:")
        print("   ğŸ“… date - Fecha (YYYY-MM-DD)")
        print("   ğŸ• hour_value - Hora del dÃ­a (0-23)")
        print("   ğŸ“Š time_period - PerÃ­odo textual (MAÃ‘ANA/TARDE/NOCHE)")
        print("   ğŸ”¤ time_period_code - CÃ³digo del perÃ­odo (MOR/AFT/NIG)")
        print("   ğŸ”— time_id - Foreign Key a dim_time.id")
        print()
        print("ğŸ“Š ESTADÃSTICAS GENERALES:")
        print(f"   â€¢ Puntos totales: {statistics['total_points']:,}")
        print(f"   â€¢ Dispositivos Ãºnicos: {statistics['unique_devices']}")
        print(f"   â€¢ BaterÃ­a promedio: {statistics['avg_battery']:.1f}%")
        print(f"   â€¢ SeÃ±al promedio: {statistics['avg_signal']:.1f} dBm")
        print(f"   â€¢ Velocidad promedio: {statistics['avg_speed']:.2f} m/s")
        print(f"   â€¢ PerÃ­odo de datos: {statistics['date_range']['start']}")
        print(f"     hasta {statistics['date_range']['end']}")
        print()
        print("ğŸ—„ï¸ TABLAS CREADAS EN BD DESTINO:")
        print("   ğŸ“‹ locations - Tabla principal con timestamp descentralizado")
        print("   ğŸ“‹ location_grid - Agregaciones por celdas geogrÃ¡ficas")
        print("   ğŸ“‹ device_stats - EstadÃ­sticas resumidas por dispositivo")
        print("   ğŸ“‹ dim_time - DimensiÃ³n de tiempo (0-23 horas)")
        print()
        print("ğŸ”¥ CONSULTAS DE ALTO RENDIMIENTO DISPONIBLES:")
        print("   âš¡ Filtros por perÃ­odo optimizados")
        print("   âš¡ Agregaciones temporales eficientes")
        print("   âš¡ Joins rÃ¡pidos con dimensiÃ³n de tiempo")
        print("="*80)

    except Exception as e:
        elapsed = time.time() - start_time
        print(f"\nâŒ ERROR EN ETL SPARK")
        print("="*50)
        print(f"âŒ Error: {e}")
        print(f"â±ï¸ Tiempo antes del fallo: {elapsed:.1f}s")

        import traceback
        traceback.print_exc()

    finally:
        if etl_service and hasattr(etl_service, 'spark') and etl_service.spark:
            etl_service.spark.stop()
            print("\nğŸ”Œ SesiÃ³n Spark finalizada")

if __name__ == "__main__":
    asyncio.run(run_spark_etl_to_database())
